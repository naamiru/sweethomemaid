{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchsampler import ImbalancedDatasetSampler\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"./data/mikans\"\n",
    "TORCH_MODEL_PATH = \"./models/mikan_classifier.pth\"\n",
    "ONNX_MODEL_PATH = \"./models/mikan_classifier.onnx\"\n",
    "\n",
    "IMAGE_SIZE = 96"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ImageFolder(\n",
    "    DATA_DIR,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    ")\n",
    "\n",
    "loader = DataLoader(\n",
    "    dataset,\n",
    "    sampler=ImbalancedDatasetSampler(dataset),\n",
    "    batch_size=1\n",
    ")\n",
    "\n",
    "mean = np.array([0.0, 0.0, 0.0])\n",
    "std = np.array([0.0, 0.0, 0.0])\n",
    "\n",
    "for image, label in loader:\n",
    "    im = np.squeeze(image.numpy())\n",
    "    mean += np.mean(im, axis=(1, 2))\n",
    "    std += np.std(im, axis=(1, 2))\n",
    "\n",
    "mean /= len(loader) \n",
    "std /= len(loader)\n",
    "\n",
    "print(\"mean:\", mean) # [0.63526792 0.57570206 0.48665065]\n",
    "print(\"std:\", std)   # [0.22508891 0.20648694 0.26393888]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize = transforms.Normalize(\n",
    "    (0.63526792, 0.57570206, 0.48665065),\n",
    "    (0.22508891, 0.20648694, 0.26393888)\n",
    ")\n",
    "unnormalize = transforms.Compose([\n",
    "    transforms.Normalize(\n",
    "        (0.0, 0.0, 0.0),\n",
    "        (1/0.22508891, 1/0.20648694, 1/0.26393888)\n",
    "    ),\n",
    "    transforms.Normalize(\n",
    "        (-0.63526792, -0.57570206, -0.48665065),\n",
    "        (1.0, 1.0, 1.0)\n",
    "    )\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ImageFolder(\n",
    "    DATA_DIR,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "        transforms.ToTensor(),\n",
    "        normalize\n",
    "    ])\n",
    ")\n",
    "idx_to_label = list(dataset.class_to_idx.keys())\n",
    "print(\"labels:\", idx_to_label)\n",
    "\n",
    "augmentated_dataset = ImageFolder(\n",
    "    DATA_DIR,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "        transforms.RandomResizedCrop(IMAGE_SIZE, scale=(0.9, 1.1), ratio=(1.0, 1,0)),\n",
    "        transforms.RandomAffine(degrees=(0.0, 0.0), translate=(0.05, 0.05), fill=(122, 111, 95)),\n",
    "        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
    "        transforms.ToTensor(),\n",
    "        normalize\n",
    "    ])\n",
    ")\n",
    "\n",
    "train_val_indices, test_indices = train_test_split(\n",
    "    list(range(len(dataset.targets))),\n",
    "    test_size=0.2,\n",
    "    stratify=dataset.targets\n",
    ")\n",
    "train_indices, val_indices = train_test_split(\n",
    "    train_val_indices,\n",
    "    test_size=0.25,\n",
    "    stratify=np.array(dataset.targets)[train_val_indices],\n",
    ")\n",
    "train_dataset = Subset(augmentated_dataset, train_indices)\n",
    "val_dataset = Subset(dataset, val_indices)\n",
    "test_dataset = Subset(dataset, test_indices)\n",
    "\n",
    "def create_loader(subset):\n",
    "    return DataLoader(\n",
    "        subset,\n",
    "        batch_size=10,\n",
    "        sampler=ImbalancedDatasetSampler(\n",
    "            subset,\n",
    "            labels=[subset.dataset.targets[i] for i in subset.indices]\n",
    "        )\n",
    "    )\n",
    "\n",
    "train_loader = create_loader(train_dataset)\n",
    "val_loader = create_loader(val_dataset)\n",
    "test_loader = create_loader(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, targets = next(iter(train_loader))\n",
    "image, target = next(zip(images, targets))\n",
    "plt.title(idx_to_label[target])\n",
    "plt.imshow(np.transpose(unnormalize(image).numpy(), (1, 2, 0)))\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, 5,),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(16, 16, 5),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2)\n",
    "        )\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(16, 32, 5),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.conv4 = nn.Sequential(\n",
    "            nn.Conv2d(32, 32, 5),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2)\n",
    "        )\n",
    "        self.fc1 = nn.Linear(10368, len(idx_to_label))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummary import summary\n",
    "summary(Net(), (3, IMAGE_SIZE, IMAGE_SIZE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "import lightning as L\n",
    "from lightning.pytorch.utilities.types import STEP_OUTPUT\n",
    "\n",
    "class PieceClassifier(L.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = Net()\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        return self.model(inputs)\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        inputs, target = batch\n",
    "        output = self(inputs)\n",
    "        loss = self.loss_fn(output, target)\n",
    "        metrics = {\"loss\": loss}\n",
    "        self.log_dict(metrics, prog_bar=True, logger=True, on_epoch=True, on_step=False)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        inputs, target = batch\n",
    "        output = self(inputs)\n",
    "        loss = self.loss_fn(output, target)\n",
    "        pred = torch.argmax(output, dim=1)\n",
    "        acc = torch.sum(pred == target) * 1.0 / len(target)\n",
    "        metrics = {\"val_loss\": loss, \"val_acc\": acc}\n",
    "        self.log_dict(metrics, prog_bar=True, logger=True, on_epoch=True, on_step=False)\n",
    "        return metrics\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        inputs, target = batch\n",
    "        output = self(inputs)\n",
    "        loss = self.loss_fn(output, target)\n",
    "        pred = torch.argmax(output, dim=1)\n",
    "        acc = torch.sum(pred == target) * 1.0 / len(target)\n",
    "        metrics = {\"test_loss\": loss, \"test_acc\": acc}\n",
    "        self.log_dict(metrics, prog_bar=True, logger=True, on_epoch=True, on_step=False)\n",
    "        return metrics\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.SGD(self.model.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "\n",
    "model = PieceClassifier()\n",
    "trainer = L.Trainer(\n",
    "    limit_train_batches=10,\n",
    "    max_epochs=100,\n",
    "    callbacks=[\n",
    "        ModelCheckpoint(\n",
    "            monitor=\"val_acc\",\n",
    "            dirpath=\"./models/checkpoints\",\n",
    "            filename=\"mikan-{epoch:02d}-{val_acc:.02f}\",\n",
    "            save_top_k=3,\n",
    "            mode=\"max\",\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "trainer.fit(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = PieceClassifier.load_from_checkpoint(\"./models/checkpoints/mikan-epoch=97-val_acc=1.00.ckpt\")\n",
    "best_model.eval()\n",
    "trainer.test(best_model, test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(best_model.model.state_dict(), TORCH_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_model = Net()\n",
    "torch_model.load_state_dict(torch.load(TORCH_MODEL_PATH))\n",
    "torch_input = next(iter(loader))[0]\n",
    "torch.onnx.export(torch_model, torch_input, ONNX_MODEL_PATH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
